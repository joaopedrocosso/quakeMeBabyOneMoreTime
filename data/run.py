import os
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from obspy import read
from dotenv import load_dotenv
from tqdm import tqdm
from obspy.core.utcdatetime import UTCDateTime
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc  # Garbage collector to release memory

# Load environment variables
load_dotenv()

# Connect to the database
def connect_db():
    conn = psycopg2.connect(
        host=os.getenv('DB_HOST'),
        database=os.getenv('DB_NAME'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD'),
        port=os.getenv('DB_PORT')
    )
    return conn

# Create tables if they don't exist, ensuring BIGINT for id and PK constraint
def create_tables(conn):
    with conn.cursor() as cur:
        cur.execute("""
        CREATE TABLE IF NOT EXISTS catalog (
            id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
            body TEXT,
            filename TEXT,
            evid TEXT,
            network TEXT,
            station TEXT,
            location TEXT,
            starttime TIMESTAMPTZ,
            endtime TIMESTAMPTZ,
            sampling_rate DOUBLE PRECISION,
            identified_arrival_time_rel DOUBLE PRECISION,
            detection_duration DOUBLE PRECISION,
            selection_duration DOUBLE PRECISION,
            file_original_size TEXT,
            file_selection_size TEXT,
            original_broadcast TEXT,
            selection_broadcast TEXT,
            features_at_detection TEXT,
            audio BYTEA
        );
        """)

        cur.execute("""
        CREATE TABLE IF NOT EXISTS data (
            id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,  -- Auto-incremento na coluna id
            evid TEXT,
            time_abs TEXT,
            time_rel TEXT,
            velocity TEXT,
            event BOOLEAN,
            catalog_id BIGINT,  -- Chave estrangeira
            CONSTRAINT fk_catalog FOREIGN KEY (catalog_id) REFERENCES catalog (id) ON DELETE CASCADE
        );
        """)

        conn.commit()

# Function to perform batch inserts
def batch_insert(cur, table_name, columns, data):
    insert_query = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s RETURNING id"
    execute_values(cur, insert_query, data)
    cur.execute('SELECT LASTVAL();')  # Retrieve the last inserted id
    return cur.fetchone()[0]

# Process CSV file in chunks and return the generated id and evid
def process_csv_in_chunks(file_path, table_name, conn, body, evid, batch_size=100000):
    print(f"Processing CSV in chunks: {file_path}")
    
    try:
        # Read CSV in chunks to avoid memory overload
        chunk_iter = pd.read_csv(file_path, chunksize=batch_size)
        
        for chunk in chunk_iter:
            # Renaming columns to remove special characters like parentheses
            chunk.columns = chunk.columns.str.replace(r'\(.*\)', '', regex=True)

            # Handle possible variations in headers and ensure they are captured
            if 'time_abs' in chunk.columns:
                chunk['time_abs'] = chunk['time_abs'].astype(str)  # Ensure TEXT format
            elif 'time' in chunk.columns:
                chunk['time_abs'] = chunk['time'].astype(str)  # Use 'time' if 'time_abs' is missing
            else:
                chunk['time_abs'] = None  # If neither exists, set as None

            if 'time_rel' in chunk.columns:
                chunk['time_rel'] = chunk['time_rel'].astype(str)  # Ensure TEXT format
            elif 'rel_time' in chunk.columns:
                chunk['time_rel'] = chunk['rel_time'].astype(str)  # Use 'rel_time' if 'time_rel' is missing
            else:
                chunk['time_rel'] = None  # If neither exists, set as None

            if 'velocity' not in chunk.columns:
                chunk['velocity'] = chunk.get('velocity', '').astype(str)  # Handling possible different header for velocity
            else:
                chunk['velocity'] = chunk['velocity'].astype(str)

            data_to_insert = []
            with conn.cursor() as cur:
                for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f"Processing {file_path}"):
                    time_abs_value = row['time_abs'] if pd.notna(row['time_abs']) else None
                    time_rel_value = row['time_rel'] if pd.notna(row['time_rel']) else None
                    velocity_value = row['velocity'] if pd.notna(row['velocity']) else None

                    data_to_insert.append((
                        evid, time_abs_value, time_rel_value, velocity_value, None
                    ))

                if data_to_insert:
                    batch_insert(cur, table_name, ['evid', 'time_abs', 'time_rel', 'velocity', 'event'], data_to_insert)
                    conn.commit()

                # Clear memory for the current chunk
                del chunk
                del data_to_insert
                gc.collect()  # Trigger garbage collection to free memory

    except pd.errors.EmptyDataError:
        print(f"Error: The CSV file {file_path} is empty or corrupted. Skipping.")
        return None, None

    return None, evid

# Process MSEED file using the same evid and id from the CSV
def process_mseed(file_path, conn, body, evid, csv_id, batch_size=100):
    print(f"Processing MSEED: {file_path}")
    st = read(file_path)
    data_to_insert = []
    with conn.cursor() as cur:
        for trace in tqdm(st, desc=f"Processing {file_path}"):
            stats = trace.stats
            starttime = stats.starttime.datetime if isinstance(stats.starttime, UTCDateTime) else stats.starttime
            endtime = stats.endtime.datetime if isinstance(stats.endtime, UTCDateTime) else stats.endtime

            # Insert with the same CSV id and evid
            data_to_insert.append((
                body, os.path.basename(file_path), evid,
                stats.network, stats.station, stats.location,
                starttime, endtime, stats.sampling_rate,
                None, None, None, None, None, None, None, None, None
            ))

            if len(data_to_insert) >= batch_size:
                batch_insert(cur, 'catalog', ['body', 'filename', 'evid', 'network', 'station', 'location', 
                                              'starttime', 'endtime', 'sampling_rate',
                                              'identified_arrival_time_rel', 'detection_duration', 
                                              'selection_duration', 'file_original_size', 
                                              'file_selection_size', 'original_broadcast', 
                                              'selection_broadcast', 'features_at_detection', 'audio'], data_to_insert)
                conn.commit()
                data_to_insert.clear()

        if data_to_insert:
            batch_insert(cur, 'catalog', ['body', 'filename', 'evid', 'network', 'station', 'location', 
                                          'starttime', 'endtime', 'sampling_rate',
                                          'identified_arrival_time_rel', 'detection_duration', 
                                          'selection_duration', 'file_original_size', 
                                          'file_selection_size', 'original_broadcast', 
                                          'selection_broadcast', 'features_at_detection', 'audio'], data_to_insert)
            conn.commit()

# Function to handle parallel processing for CSV and MSEED files
def process_file_pair(file_pair, body, batch_size):
    csv_file = file_pair['csv']
    mseed_file = file_pair['mseed']
    evid = csv_file.split('_evid')[-1].replace('.csv', '')

    conn = connect_db()
    try:
        process_csv_in_chunks(csv_file, 'data', conn, body, evid, batch_size)
        process_mseed(mseed_file, conn, body, evid, None, batch_size)
    finally:
        conn.close()

# Traverse directories and process files in parallel with 4 connections
def process_directory_parallel(directory, body, batch_size=100000):
    file_pairs = {}

    # Recursively find all .csv and .mseed files in the directory
    for root, _, filenames in os.walk(directory):
        for file in filenames:
            file_path = os.path.join(root, file)
            base_filename = file.split('_evid')[0]

            if file.endswith('.csv'):
                if base_filename not in file_pairs:
                    file_pairs[base_filename] = {}
                file_pairs[base_filename]['csv'] = file_path
            elif file.endswith('.mseed'):
                if base_filename not in file_pairs:
                    file_pairs[base_filename] = {}
                file_pairs[base_filename]['mseed'] = file_path

    # Parallel processing of file pairs using ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_file_pair, files, body, batch_size) for files in file_pairs.values()]
        for future in as_completed(futures):
            try:
                future.result()  # This will raise an exception if the thread encountered an error
            except Exception as exc:
                print(f"An error occurred: {exc}")

# Main function
def main():
    conn = connect_db()
    create_tables(conn)
    conn.close()

    directories = {
        'mars': './space_apps_2024_seismic_detection/data/mars/test/data',
        'lunar': './space_apps_2024_seismic_detection/data/lunar/test/data'
    }

    batch_size = 100000

    for body, directory in directories.items():
        print(f"Processing body: {body}")
        process_directory_parallel(directory, body, batch_size)

if __name__ == '__main__':
    main()
