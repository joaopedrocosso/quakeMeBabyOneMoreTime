{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280068ac-6f5b-46a2-b53d-a615824513a0",
   "metadata": {},
   "source": [
    "# Library Import\n",
    "This section imports all the necessary libraries required for data processing, feature extraction, and cloud interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31597c58-2147-49ca-8953-0fa00885944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from utils.feature_extraction import *\n",
    "from timeit import default_timer as timer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "# from google.cloud import storage # Uncomment this line if you want to use Cloud Storage to upload the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28c12e-7a84-4629-a482-94356efde40f",
   "metadata": {},
   "source": [
    "# Function to Parallelize Event Processing (CSV and mseed)\n",
    "- This notebook was executed on Google Cloud, in a Vertex AI instance with 64 vCPUs and 64GB of RAM, to enable accelerated data processing.\n",
    "- All data from the CSV files and the statuses from the mseed files are being extracted.\n",
    "- **Additionally, we have a utility file for creating new event features using mathematical equations with the Scipy library; all these new features are utilized in training the neural network.**\n",
    "Description:\n",
    "\n",
    "## Each row represents an event from the lunar dataset. For each event:\n",
    "- CSV data is loaded and processed. **We are adding the filename to the dataframe**.\n",
    "- Data from mseed files (including network and station status) is extracted.\n",
    "- The CSV and mseed data are combined and returned as a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b95e3-95a0-45c1-b745-ec7eaa53ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each row (parallelizable function)\n",
    "def process_row(filename):\n",
    "    start = timer()\n",
    "    data_directory = f'./data/lunar/test/{filename}'\n",
    "\n",
    "    try:\n",
    "        # Read CSV data\n",
    "        csv_path = f\"{data_directory}.csv\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File {csv_path} not found, skipping row.\",end='\\n')\n",
    "            return None\n",
    "\n",
    "        df_data_csv = pd.read_csv(csv_path, parse_dates=['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "        df_data_csv['filename']=filename\n",
    "\n",
    "        # Concatenate features from mseed\n",
    "        mseed_file = f'{data_directory}.mseed'\n",
    "        st = read(mseed_file)\n",
    "        df_data_csv['network'] = st[0].stats['network']\n",
    "        df_data_csv['station'] = st[0].stats['station']\n",
    "        df_data_csv['location'] = st[0].stats['location']\n",
    "        df_data_csv['channel'] = st[0].stats['channel']\n",
    "        df_data_csv['sampling_rate'] = st[0].stats['sampling_rate']\n",
    "        df_data_csv['delta'] = st[0].stats['delta']\n",
    "        df_data_csv['npts'] = st[0].stats['npts']\n",
    "        df_data_csv['calib'] = st[0].stats['calib']\n",
    "\n",
    "        # Concatenating additional features\n",
    "        sampling_rate = st[0].stats['sampling_rate']\n",
    "        features = process_seismic_data(df_data_csv, sampling_rate)\n",
    "        df_data_csv['mean_velocity'] = features['mean_velocity']\n",
    "        df_data_csv['std_velocity'] = features['std_velocity']\n",
    "        df_data_csv['max_velocity'] = features['max_velocity']\n",
    "        df_data_csv['min_velocity'] = features['min_velocity']\n",
    "        df_data_csv['total_energy'] = features['total_energy']\n",
    "        df_data_csv['rms_value'] = features['rms_value']\n",
    "        df_data_csv['peak_count'] = features['peak_count']\n",
    "        df_data_csv['valley_count'] = features['valley_count']\n",
    "        df_data_csv['fft_values'] = features['fft_values']\n",
    "        df_data_csv['fft_freqs'] = features['fft_freqs']\n",
    "        df_data_csv['autocorrelation'] = features['autocorrelation']\n",
    "        df_data_csv['acceleration'] = features['acceleration']\n",
    "        df_data_csv['jerk'] = features['jerk']\n",
    "        df_data_csv['cumulative_energy'] = features['cumulative_energy']\n",
    "\n",
    "        end = timer()  # Stop the timer\n",
    "        elapsed_time = end - start  # Calculate elapsed time\n",
    "        print(f\"Processing row took {elapsed_time:.4f} seconds\", end='\\n')\n",
    "\n",
    "        return df_data_csv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row with filename {filename}: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "# Function to handle parallel processing\n",
    "def parallel_process():\n",
    "    folder_path = './data/lunar/test'\n",
    "    files = {os.path.splitext(f)[0] for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))}\n",
    "    \n",
    "    start2 = timer()\n",
    "    # Create a pool of workers equal to the number of CPU cores\n",
    "    pool = Pool(cpu_count())\n",
    "\n",
    "    # Process rows in parallel and gather results\n",
    "    result_dfs = pool.map(process_row, [file for file in files])\n",
    "\n",
    "    # Close the pool and wait for the work to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Filter out None results before concatenating\n",
    "    result_dfs = [df for df in result_dfs if df is not None]\n",
    "\n",
    "    # Combine all results into a single DataFrame if there is valid data\n",
    "    if result_dfs:\n",
    "        df_combined = pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        df_combined = pd.DataFrame()  # Return an empty DataFrame if nothing is processed\n",
    "\n",
    "    end2 = timer()  # Stop the timer\n",
    "    elapsed_time = end2 - start2  # Calculate elapsed time\n",
    "    print(f\"Final process time took {elapsed_time:.4f} seconds\", end='\\n')\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d18fa-5e23-41ea-ba6f-771484c41b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=parallel_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08848487-c3b5-422f-9c7f-2e06f338888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"./test_lunar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98655b16-91c1-4e03-ae93-3d11ce2bcac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5c01d-1b6c-44e8-b748-dac8e59aa91e",
   "metadata": {},
   "source": [
    "# Saving the File to the Cloud for Faster Download When Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be11a1-b058-448a-a310-f1cfe3576b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    os.remove(source_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247c77d-489c-4a49-a693-871d5e778de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcs(\"my-gcp-bucket\",\"./test_lunar.csv\",\"test_lunar.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
