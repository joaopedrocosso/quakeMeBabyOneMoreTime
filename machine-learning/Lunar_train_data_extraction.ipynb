{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037cde1b-8c9a-4aa3-8fca-0488bacee389",
   "metadata": {},
   "source": [
    "# Library Import\n",
    "This section imports all the necessary libraries required for data processing, feature extraction, and cloud interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cb9bd8-39c6-4104-9091-9eb57908c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from utils.feature_extraction import *\n",
    "from timeit import default_timer as timer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "# from google.cloud import storage # Uncomment this line if you want to use Cloud Storage to upload the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad9c73-0d5a-4fa5-988a-fb8fda98ab38",
   "metadata": {},
   "source": [
    "# Loading the Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42535c-be6d-4c4b-ba94-8efe49f36d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = pd.read_csv('./data/lunar/training/catalogs/apollo12_catalog_GradeA_final.csv')\n",
    "df_geral.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b2235-7f5f-43c4-8ff7-083a5dbb0e34",
   "metadata": {},
   "source": [
    "# Function to Parallelize Event Processing (CSV and mseed)\n",
    "- This notebook was executed on Google Cloud, in a Vertex AI instance with 64 vCPUs and 64GB of RAM, to enable accelerated data processing.\n",
    "- All data from the CSV files and the statuses from the mseed files are being extracted.\n",
    "- **Additionally, we have a utility file for creating new event features using mathematical equations with the Scipy library; all these new features are utilized in training the neural network.**\n",
    "Description:\n",
    "\n",
    "## Each row represents an event from the lunar dataset. For each event:\n",
    "- CSV data is loaded and processed. **We are adding the filename and a label to the dataframe**.\n",
    "- Data from mseed files (including network and station status) is extracted.\n",
    "- The CSV and mseed data are combined and returned as a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0e090-4bf7-4412-879c-696beb22b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each row (parallelizable function)\n",
    "def process_row(row):\n",
    "    start = timer()\n",
    "    filename = row['filename']\n",
    "    time_rel_label = row['time_rel(sec)']\n",
    "    data_directory = f'./data/lunar/training/data/{filename}'\n",
    "\n",
    "    try:\n",
    "        # Read CSV data\n",
    "        csv_path = f\"{data_directory}.csv\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File {csv_path} not found, skipping row.\",end='\\n')\n",
    "            return None\n",
    "\n",
    "        df_data_csv = pd.read_csv(csv_path, parse_dates=['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "        df_data_csv['filename'] = filename\n",
    "        df_data_csv['label'] = (df_data_csv['time_rel(sec)'] >= time_rel_label).astype(int)  # Create label\n",
    "\n",
    "        # Concatenate features from mseed\n",
    "        mseed_file = f'{data_directory}.mseed'\n",
    "        st = read(mseed_file)\n",
    "        df_data_csv['network'] = st[0].stats['network']\n",
    "        df_data_csv['station'] = st[0].stats['station']\n",
    "        df_data_csv['location'] = st[0].stats['location']\n",
    "        df_data_csv['channel'] = st[0].stats['channel']\n",
    "        df_data_csv['sampling_rate'] = st[0].stats['sampling_rate']\n",
    "        df_data_csv['delta'] = st[0].stats['delta']\n",
    "        df_data_csv['npts'] = st[0].stats['npts']\n",
    "        df_data_csv['calib'] = st[0].stats['calib']\n",
    "\n",
    "        # Concatenating additional features\n",
    "        sampling_rate = st[0].stats['sampling_rate']\n",
    "        features = process_seismic_data(df_data_csv, sampling_rate)\n",
    "        df_data_csv['mean_velocity'] = features['mean_velocity']\n",
    "        df_data_csv['std_velocity'] = features['std_velocity']\n",
    "        df_data_csv['max_velocity'] = features['max_velocity']\n",
    "        df_data_csv['min_velocity'] = features['min_velocity']\n",
    "        df_data_csv['total_energy'] = features['total_energy']\n",
    "        df_data_csv['rms_value'] = features['rms_value']\n",
    "        df_data_csv['peak_count'] = features['peak_count']\n",
    "        df_data_csv['valley_count'] = features['valley_count']\n",
    "        df_data_csv['fft_values'] = features['fft_values']\n",
    "        df_data_csv['fft_freqs'] = features['fft_freqs']\n",
    "        df_data_csv['autocorrelation'] = features['autocorrelation']\n",
    "        df_data_csv['acceleration'] = features['acceleration']\n",
    "        df_data_csv['jerk'] = features['jerk']\n",
    "        df_data_csv['cumulative_energy'] = features['cumulative_energy']\n",
    "\n",
    "        end = timer()  # Stop the timer\n",
    "        elapsed_time = end - start  # Calculate elapsed time\n",
    "        print(f\"Processing row took {elapsed_time:.4f} seconds\", end='\\n')\n",
    "\n",
    "        return df_data_csv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row with filename {filename}: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "# Function to handle parallel processing\n",
    "def parallel_process(df_geral):\n",
    "    start2 = timer()\n",
    "    # Create a pool of workers equal to the number of CPU cores\n",
    "    pool = Pool(cpu_count())\n",
    "\n",
    "    # Process rows in parallel and gather results\n",
    "    result_dfs = pool.map(process_row, [row for index, row in df_geral.iterrows()])\n",
    "\n",
    "    # Close the pool and wait for the work to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Filter out None results before concatenating\n",
    "    result_dfs = [df for df in result_dfs if df is not None]\n",
    "\n",
    "    # Combine all results into a single DataFrame if there is valid data\n",
    "    if result_dfs:\n",
    "        df_combined = pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        df_combined = pd.DataFrame()  # Return an empty DataFrame if nothing is processed\n",
    "\n",
    "    end2 = timer()  # Stop the timer\n",
    "    elapsed_time = end2 - start2  # Calculate elapsed time\n",
    "    print(f\"Final process time took {elapsed_time:.4f} seconds\", end='\\n')\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88945e00-2ed0-42f8-a670-0bb6cbee156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=parallel_process(df_geral)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627a532-974a-43e9-9a3c-4b14c7f86544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"./training_lunar.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706daebc-a9b7-4d4e-92a1-951d3471ea9c",
   "metadata": {},
   "source": [
    "# Saving the File to the Cloud for Faster Download When Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1471b5-4fab-44c4-a269-3ea89a5c0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para fazer upload de arquivos para o Google Cloud Storage\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)  # Faz upload do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b2338-1edf-46c9-9fa0-687f7865937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcs(\"my-gcp-bucket\",\"./training_lunar.csv\",\"training_lunar.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
