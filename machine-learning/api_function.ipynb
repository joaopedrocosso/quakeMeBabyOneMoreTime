{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d980a6e6-5fc7-4c0a-9c5e-c415d3bd3346",
   "metadata": {},
   "source": [
    "# Library Import\n",
    "This section imports all the necessary libraries required for data processing, feature extraction, and cloud interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260c14c8-a3eb-4985-8117-af26b9de8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.feature_extraction import *\n",
    "import copy\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a82fe3-06a7-4467-8952-b4346d465436",
   "metadata": {},
   "source": [
    "# Function to allow a custom prediction on our website\n",
    "- This function remove any null data from the csv that we'll get from the user **(Attention point from the challenge webpage)**\n",
    "- The function also verify if the user's csv have the minimum fields to generate a prediction\n",
    "\n",
    "## Creating aditional features for the output catalog that the webpage'll show on the user's screen:\n",
    "- identified_arrival_time_rel(sec): time_rel(s) in the first row considered an earthquake\n",
    "- detection_duration(sec): time_rel(s) in the last row of the entire subset of a record\n",
    "- selection_duration(sec): detection_duration(sec) - identified_arrival_time_rel(sec)\n",
    "- features_at_detection: the features in the first row considered an earthquake\n",
    "- file_original_size(kb): pandas.memory_usage() method on the complete subset of a detection\n",
    "- file_selection_size(kb): pandas.memory_usage() method on the subset trimmed from the detection index to the end\n",
    "- original_broadcast: file_original_size(kb) / transmission rate\n",
    "- selection_broadcast: file_selection_size(kb) / transmission rate\n",
    "\n",
    "- Data transmission rates:\n",
    "\n",
    "    - Apollo 12: 51.2 kbps\n",
    "    - Apollo 15: 85.6 kbps\n",
    "    - Apollo 16: 85.6 kbps\n",
    "    - InSight: 256 kbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef9c4c8-fa6f-4182-93de-7a538e2dc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename):\n",
    "    import json\n",
    "    #### Colocar aqui o recebimento do CSV e transformar ele em dataframe:\n",
    "    df_data_csv = pd.read_csv('./example_data/'+filename+'.csv') # pegar da api\n",
    "    \n",
    "    sampling_rate = 6.625 #pegar da api, é um valor float\n",
    "    \n",
    "    columns_to_check = ['time_rel(sec)', 'velocity(m/s)']\n",
    "    missing_columns = [col for col in columns_to_check if col not in df_data_csv.columns]\n",
    "    df_data_csv.dropna(inplace=True)\n",
    "    \n",
    "    if len(missing_columns)>0:\n",
    "        return \"Error, time_rel(sec) and velocity(m/s) columns are missing on the csv\"\n",
    "\n",
    "    # Concatenando novas features\n",
    "    df_data_csv['filename']=filename\n",
    "    features = process_seismic_data(df_data_csv, sampling_rate)\n",
    "    df_data_csv['mean_velocity']= features['mean_velocity']\n",
    "    df_data_csv['std_velocity']= features['std_velocity']\n",
    "    df_data_csv['max_velocity']= features['max_velocity']\n",
    "    df_data_csv['min_velocity']= features['min_velocity']\n",
    "    df_data_csv['total_energy']= features['total_energy']\n",
    "    df_data_csv['rms_value']= features['rms_value']\n",
    "    df_data_csv['peak_count']= features['peak_count']\n",
    "    df_data_csv['valley_count']= features['valley_count']\n",
    "    df_data_csv['fft_values']= features['fft_values']\n",
    "    df_data_csv['fft_freqs']= features['fft_freqs']\n",
    "    df_data_csv['autocorrelation']= features['autocorrelation']\n",
    "    df_data_csv['acceleration']= features['acceleration']\n",
    "    df_data_csv['jerk']= features['jerk']\n",
    "    df_data_csv['cumulative_energy']= features['cumulative_energy']\n",
    "    \n",
    "    \n",
    "    temp = copy.deepcopy(df_data_csv)\n",
    "    temp.drop(['time_abs(%Y-%m-%dT%H:%M:%S.%f)','filename'],axis=1,inplace=True)\n",
    "    \n",
    "    model = load_model('./model/best_model_nasa.keras')\n",
    "    y_pred=model.predict(temp,verbose=2)\n",
    "    df_data_csv['y_pred'] = (y_pred > 0.5).astype(int)  # Convert to 0 or 1\n",
    "\n",
    "    #Generating output features\n",
    "    result = df_data_csv[df_data_csv['y_pred'] == 1].groupby('filename', as_index=False).nth(0).reset_index()\n",
    "    memory_usage_per_group = df_data_csv[df_data_csv['y_pred'] == 1].groupby('filename').apply(lambda group: group.memory_usage(deep=True).sum())\n",
    "    result['file_selection_size(kb)'] = result['filename'].map(memory_usage_per_group)\n",
    "    \n",
    "    features = result.drop(['file_selection_size(kb)'],axis=1)\n",
    "    \n",
    "    json_list = []\n",
    "    for index, row in features.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        \n",
    "        # Convert complex numbers to strings\n",
    "        for key, value in row_dict.items():\n",
    "            if isinstance(value, complex):\n",
    "                row_dict[key] = str(value)\n",
    "        \n",
    "        json_list.append(json.dumps(row_dict))\n",
    "        \n",
    "    result = result[['filename', 'time_rel(sec)','velocity(m/s)','index','file_selection_size(kb)']].rename(columns={\"index\":\"index_predict\",\"time_rel(sec)\":\"identified_arrival_time_rel(sec)\"})\n",
    "    \n",
    "    result2 = df_data_csv.groupby('filename', as_index=False).tail(1).reset_index()[['filename','index', 'time_rel(sec)']].rename(\n",
    "        columns={\"index\": \"index_tail\", \"time_rel(sec)\": \"detection_duration(sec)\"}\n",
    "    )\n",
    "    memory_usage_per_group = df_data_csv.groupby('filename').apply(lambda group: group.memory_usage(deep=True).sum())\n",
    "    result2['file_original_size(kb)'] = result2['filename'].map(memory_usage_per_group)\n",
    "\n",
    "    result3 = pd.concat([result, result2.drop(['filename'],axis=1)],axis=1)\n",
    "    result3['selection_duration'] = result3['detection_duration(sec)'] - result3['identified_arrival_time_rel(sec)']\n",
    "    result3['features']=json_list\n",
    "    conditions = [\n",
    "        result3['filename'].str.contains('s12'),\n",
    "        result3['filename'].str.contains('s15'),\n",
    "        result3['filename'].str.contains('s16')\n",
    "    ]\n",
    "    values = [51.2, 85.6, 85.6]\n",
    "    result3['transmission_speed'] = np.select(conditions, values, default=256)\n",
    "    result3['original_broadcast'] = result3['file_original_size(kb)'] / result3['transmission_speed']\n",
    "    result3['selection_broadcast'] = result3['file_selection_size(kb)'] / result3['transmission_speed']\n",
    "    result3.drop(['index_predict','index_tail','transmission_speed'],inplace=True,axis=1)\n",
    "    return result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba1a0fa-c81b-43d7-8f02-9aaaa62895d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17888/17888 - 17s - 934us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitor\\AppData\\Local\\Temp\\ipykernel_45856\\3132852625.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  memory_usage_per_group = df_data_csv[df_data_csv['y_pred'] == 1].groupby('filename').apply(lambda group: group.memory_usage(deep=True).sum())\n",
      "C:\\Users\\vitor\\AppData\\Local\\Temp\\ipykernel_45856\\3132852625.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  memory_usage_per_group = df_data_csv.groupby('filename').apply(lambda group: group.memory_usage(deep=True).sum())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>identified_arrival_time_rel(sec)</th>\n",
       "      <th>velocity(m/s)</th>\n",
       "      <th>file_selection_size(kb)</th>\n",
       "      <th>detection_duration(sec)</th>\n",
       "      <th>file_original_size(kb)</th>\n",
       "      <th>selection_duration</th>\n",
       "      <th>features</th>\n",
       "      <th>original_broadcast</th>\n",
       "      <th>selection_broadcast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xa.s12.00.mhz.1970-01-19HR00_evid00002</td>\n",
       "      <td>41071.849057</td>\n",
       "      <td>-1.346839e-10</td>\n",
       "      <td>97902364</td>\n",
       "      <td>86402.113208</td>\n",
       "      <td>186607290</td>\n",
       "      <td>45330.264151</td>\n",
       "      <td>{\"index\": 272101, \"time_abs(%Y-%m-%dT%H:%M:%S....</td>\n",
       "      <td>3.644674e+06</td>\n",
       "      <td>1.912156e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  identified_arrival_time_rel(sec)  \\\n",
       "0  xa.s12.00.mhz.1970-01-19HR00_evid00002                      41071.849057   \n",
       "\n",
       "   velocity(m/s)  file_selection_size(kb)  detection_duration(sec)  \\\n",
       "0  -1.346839e-10                 97902364             86402.113208   \n",
       "\n",
       "   file_original_size(kb)  selection_duration  \\\n",
       "0               186607290        45330.264151   \n",
       "\n",
       "                                            features  original_broadcast  \\\n",
       "0  {\"index\": 272101, \"time_abs(%Y-%m-%dT%H:%M:%S....        3.644674e+06   \n",
       "\n",
       "   selection_broadcast  \n",
       "0         1.912156e+06  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"xa.s12.00.mhz.1970-01-19HR00_evid00002\" #Receber o nome do arquivo da api\n",
    "result = predict(filename) #Pode passar tanto o filename quanto ajustar para já passar o dataframe direto, o que for melhor\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e31b1e-b56f-4f07-a971-cbe04a9da999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
